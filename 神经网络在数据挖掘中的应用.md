# 神经网络在数据挖掘中的应用

## 摘要

还没写

**关键词**：神经网络 数据挖掘 深度学习

## 线性模型

线性模型可以视为最简单的神经网络，本文将从线性模型开始介绍神经网络模型。

### 多元线性回归

设有数据集 $\{x_i,y_i\}_{i=1}^m$，其中 $x_i=(x_{i1},x_{i2},\cdots,x_{in})\in \mathbb{R}^n,y_i\in \mathbb{R}$，$y_i$ 与 $x_i$ 有潜在的关系 $y_i=f(x_i)$，我们的目标就是找到这样的关系 $f$，使得 $\hat{y}_i=f(x_i)$ 与 $y_i$ 的差别尽可能小，也就可以用 $f$ 来对 $y_i$ 进行预测。

（为表述方便，下用 $x,y$ 表示上文中的 $x_i,y_i$）

在**多元线性回归**中，假定 $y$ 与 $x$ 的关系是一个线性表达式，即
$$
\hat{y}=\sum_{j=1}^nw_jx_{j}+b
$$
写成向量形式，即
$$
\hat{y}=wx^T+b
$$
其中 $w=(w_1,w_2,\cdots,w_n)$，称 $w$ 为权重，$b$ 为阈值。

模型求解的过程分为训练过程和预测过程，训练过程即通过 $x,y$ 求 $w$ 与 $b$，预测过程即通过 $x,w,b$ 求 $\hat{y}$。

### 对数几率回归（逻辑回归）

在上述问题中，有一类特殊的问题，即概率问题，当我们需要预测的 $y$ 为概率时，我们可以对数据属于两类的条件概率的比值进行预测，假设

$$
\ln \frac{P(y=1|x)}{P(y=0|x)}=\sum_{j=1}^nw_jx_j+b
$$
同时显然有
$$
P(y=1|x)+P(y=0|x)=1
$$
解得
$$
P(y=1|x)=\frac{1}{1+e^{-(wx^T+b)}}
$$

$$
P(y=0|x)=\frac{1}{1+e^{wx^T+b}}
$$

因此，我们可以用 $\hat{y}_i=P(y=1|x)=\frac{1}{1+e^{-(wx^T+b)}}$ 来作为 $y_i$ 的预测值

定义 sigmoid 函数
$$
\text{sigmoid}(x)=\frac{1}{1+e^{-x}}
$$
进而得到**对数几率回归**模型的表达式
$$
y=\text{sigmoid}(wx^T+b)
$$
与多元线性回归类似，模型求解的过程分为训练过程和预测过程，训练过程即通过 $x,y$ 求 $w$ 与 $b$，预测过程即通过 $x,w,b$ 求 $\hat{y}$。

### 梯度下降法

要进行模型的训练，需要有一个指标来衡量模型参数 $w,b$ 的优劣，也就是损失函数，例如**均方误差**（mean-square error），即
$$
E(x,y,w,b)=\frac{1}{2}(\hat{y}-y)^2
$$
显然，损失函数的值越小，$\hat{y}$ 与 $y$ 的差异就越小，模型的效果就越好。

均方误差函数是一个凸函数，因此可以使用**梯度下降法**求解 $w,b$。梯度是函数变化最快的方向，向梯度的反方向移动 $w$ 与 $b$，将会逐渐逼近最优解，即先随机确定 $w^{[0]},b^{[0]}$ ，再以下面的公式进行迭代
$$
w^{[n+1]}=w^{[n]}-\eta\frac{\partial E}{\partial w^{[n]}}
$$
$$
b^{[n+1]}=b^{[n]}-\eta\frac{\partial E}{\partial b^{[n]}}
$$

经过若干次迭代后将收敛到最优的 $w,b$。其中 $\eta$ 是常参数，称为学习率，具体数值根据实际问题调整。

### 使用线性模型进行手写数字识别

我们在数据挖掘网站 Kaggle 上获取 MNIST 手写数字识别数据集，该数据集中包含 $42000$ 个数据，每个数据包含 $28\times 28$ 的图像灰度值数据，范围为 $[0,255]$，以及一个标签 $d$，表示图像对应的数字。首先我们将图像的灰度值数据变成一个 $784$ 维的向量，对应线性模型中的 $n=784$。然后将整个数据集划分为训练数据集和测试数据集，也就是将整个数据集随机打乱顺序后，将前 $30000$ 个数据作为训练数据集，用来训练模型，后 $12000$ 个数据作为测试数据集，用来测试模型的准确率。

在训练前，将标签转换为 one-hot 编码，即 $y=(y_0,y_1,\cdots,y_9)$，其中 $y_d=1$，其余全部为 $0$；将灰度值除以 $255$，映射到范围 $[0,1]$ 中；用 $10$ 个独立的线性模型来拟合 $y_0,y_1,\cdots,y_9$，在预测时，取
$$
\hat{d}=\arg \max_i \hat{y}_i
$$
作为预测结果；在训练时，每次随机取一个数据，用梯度下降法进行迭代。

首先测试多元线性回归，$w$ 与 $b$ 在范围 $[-0.5,0.5]$ 内以均匀分布进行随机初始化，学习率 $\eta=0.001$，训练足够多次后，准确率达到 $83\%$ 左右。

![](C:\git\bs\figure\多元线性回归准确率.jpg)

然后测试对数几率回归，$w$ 与 $b$ 在范围 $[-0.5,0.5]$ 内以均匀分布进行随机初始化，学习率 $\eta=0.1$，训练足够多次后，准确率达到 $90\%$ 左右。

![](C:\git\bs\figure\对数几率回归准确率.jpg)

为了了解模型的权重分布情况，我们将 $w$ 进行可视化，下图是多元线性回归的权重图，权重越大，对应位置的颜色越亮

![](C:\git\bs\figure\多元线性回归权值.BMP)

从图中可以看出，在图像边缘部分权重的分布看起来杂乱无章，是因为图像的分类由中心位置的灰度值决定。

下图是对数几率回归的权重分布图，在这张图中可以隐约看到 $10$ 个数字的轮廓，不同数字的边缘部分明暗不一是由模型中常数项 $b$ 的不同导致的。

![](C:\git\bs\figure\对数几率回归权值.jpg)

## 神经网络模型

我们将线性模型抽象成神经网络模型，线性模型可以视为 $n+1$ 个神经元构成的神经网络，包括 $n$ 个输入神经元构成的输入层，与 $1$ 个输出神经元构成的输出层，每个神经元有输入值和输出值，这些值通过神经网络上的有向边来传递。两个线性模型的唯一不同之处是对数几率回归的输出神经元上添加了 sigmoid 函数。

![](C:\git\bs\figure\simple_net.jpg)

### 感知机

将线性模型变得稍加复杂一点，在输入层与输出层之间添加隐含层，包含若干个神经元，隐含层的每个神经元是输入层神经元输出值的线性组合+该神经元的阈值，输出层神经元是隐含层神经元输出值的线性组合+该神经元的阈值。当然，输出层可以包含多个神经元，也就可以输出多个值。

![](C:\git\bs\figure\感知机.jpg)

然而，这样的模型仅仅是在结构上变得复杂了一点，输出的结果仍然是输入值的线性组合+阈值，效果与线性模型无异，为此，需要在隐含层与输出层神经元上添加激活函数，让模型的表达式变成非线性表达式。

#### 激活函数

在对数几率回归中的 sigmoid 函数就是一个激活函数，添加了激活函数的神经元，其输出值为输入值作用激活函数之后的值，常用的激活函数有

| 激活函数     | 函数表达式                               |
| ------------ | ---------------------------------------- |
| sigmoid 函数 | $f(x)=\frac{1}{1+e^{-x}}$                |
| relu 函数    | $f(x)=\max(x,0)$                         |
| softmax 函数 | $f(x_i)=\frac{e^{x_i}}{\sum_{j}e^{x_j}}$ |

#### 误差逆传播算法

对于神经网络模型，在训练过程中，继续使用梯度下降法，考虑到模型结构的复杂性，我们把神经网络抽象成一个有向无环图 $G=(V,E)$，其中 $V$ 是所有神经元的集合，$E$ 是所有有向边的集合，我们可以先计算损失函数对于每个神经元输出值的偏导数，进而求出损失函数对每个权重和阈值的偏导数

**定义 拓扑排序** 在有向无环图 $G=(V,E)$ 中，设 $P=(u_1,u_2,\cdots,u_n)$ 是顶点集 $V$ 的一个排列，若任意 $<u,v>\in E$，$u$ 在 $P$ 中排在 $v$ 前面，则称 $P$ 是 $G$ 的一个拓扑排序。

```python
# 正向传播
对 V 进行拓扑排序
for u in V:
    for e = <u, v> in E:
        传递 u 的输出值到 v 的输入值
# 将所有边反向
for e = <u, v> in E:
    e = <v, u> 
# 反向传播
对 V 进行拓扑排序
计算损失函数对输出层神经元输出值的导数值
for u in V:
    for e = <u, v> in E:
        传递 u 的导数值到 v 的导数值
        更新 e 的权重
    更新 u 的阈值（如果有的话）
```

#### 使用单隐层感知机进行手写数字识别

以手写数字识别为例，在隐含层构造 $30$ 个神经元，输出层构造 $10$ 个神经元，所有权重与阈值在范围 $[-0.5,0.5]$ 中以均匀分布随机初始化，学习率 $\eta=0.1$，经过足够多次迭代后，准确率达到 $94\%$ 左右。

![](C:\git\bs\figure\感知机准确率.jpg)

单隐层感知机相比于线性模型，在准确率上有了提升，但是模型的可解释性降低。

### 卷积神经网络（CNN）

#### 卷积层与池化层

#### 使用卷积神经网络进行手写数字识别

### 循环神经网络（RNN）

#### LSTM

### 生成对抗网络（GAN）

## 集成算法

### AdaBoost算法

## 参考文献





