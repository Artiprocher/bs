# 神经网络在数据挖掘中的应用

## 摘要

还没写

**关键词**：神经网络 数据挖掘 深度学习

## 线性模型

线性模型可以视为最简单的神经网络，本文将从线性模型开始介绍神经网络模型。

### 多元线性回归

设有数据集 $\{x_i,y_i\}_{i=1}^m$，其中 $x_i=(x_{i1},x_{i2},\cdots,x_{in})\in \mathbb{R}^n,y_i\in \mathbb{R}$，$y_i$ 与 $x_i$ 有潜在的关系 $y_i=f(x_i)$，我们的目标就是找到这样的关系 $f$，使得 $\hat{y}_i=f(x_i)$ 与 $y_i$ 的差别尽可能小，也就可以用 $f$ 来对 $y_i$ 进行预测。

（为表述方便，下用 $x,y$ 表示上文中的 $x_i,y_i$）

在**多元线性回归**中，假定 $y$ 与 $x$ 的关系是一个线性表达式，即
$$
\hat{y}=\sum_{j=1}^nw_jx_{j}+b
$$
写成向量形式，即
$$
\hat{y}=wx^T+b
$$
其中 $w=(w_1,w_2,\cdots,w_n)$，称 $w$ 为权重，$b$ 为阈值。

模型求解的过程分为训练过程和预测过程，训练过程即通过 $x,y$ 求 $w$ 与 $b$，预测过程即通过 $x,w,b$ 求 $\hat{y}$。

### 对数几率回归（逻辑回归）

在上述问题中，有一类特殊的问题，即概率问题，当我们需要预测的 $y$ 为概率时，我们可以对数据属于两类的条件概率的比值进行预测，假设

$$
\ln \frac{P(y=1|x)}{P(y=0|x)}=\sum_{j=1}^nw_jx_j+b
$$
同时显然有
$$
P(y=1|x)+P(y=0|x)=1
$$
解得
$$
P(y=1|x)=\frac{1}{1+e^{-(wx^T+b)}}
$$

$$
P(y=0|x)=\frac{1}{1+e^{wx^T+b}}
$$

因此，我们可以用 $\hat{y}_i=P(y=1|x)=\frac{1}{1+e^{-(wx^T+b)}}$ 来作为 $y_i$ 的预测值

定义 sigmoid 函数
$$
\text{sigmoid}(x)=\frac{1}{1+e^{-x}}
$$
进而得到**对数几率回归**模型的表达式
$$
y=\text{sigmoid}(wx^T+b)
$$
与多元线性回归类似，模型求解的过程分为训练过程和预测过程，训练过程即通过 $x,y$ 求 $w$ 与 $b$，预测过程即通过 $x,w,b$ 求 $\hat{y}$。

### 梯度下降法

要进行模型的训练，需要有一个指标来衡量模型参数 $w,b$ 的优劣，也就是损失函数，例如**均方误差**（mean-square error），即
$$
E(x,y,w,b)=\frac{1}{2}(\hat{y}-y)^2
$$
显然，损失函数的值越小，$\hat{y}$ 与 $y$ 的差异就越小，模型的效果就越好。

均方误差函数是一个凸函数，因此可以使用**梯度下降法**求解 $w,b$。梯度是函数变化最快的方向，向梯度的反方向移动 $w$ 与 $b$，将会逐渐逼近最优解，即先随机确定 $w^{[0]},b^{[0]}$ ，再以下面的公式进行迭代
$$
w^{[n+1]}=w^{[n]}-\eta\frac{\partial E}{\partial w^{[n]}}
$$
$$
b^{[n+1]}=b^{[n]}-\eta\frac{\partial E}{\partial b^{[n]}}
$$

经过若干次迭代后将收敛到最优的 $w,b$。其中 $\eta$ 是常参数，称为学习率，具体数值根据实际问题调整。

### 使用线性模型进行手写数字识别

我们在数据挖掘网站 Kaggle 上获取 MNIST 手写数字识别数据集，该数据集中包含 $42000$ 个数据，每个数据包含 $28\times 28$ 的图像灰度值数据，范围为 $[0,255]$，以及一个标签 $d$，表示图像对应的数字。首先我们将图像的灰度值数据变成一个 $784$ 维的向量，对应线性模型中的 $n=784$。然后将整个数据集划分为训练数据集和测试数据集，也就是将整个数据集随机打乱顺序后，将前 $30000$ 个数据作为训练数据集，用来训练模型，后 $12000$ 个数据作为测试数据集，用来测试模型的准确率。

在训练前，将标签转换为 one-hot 编码，即 $y=(y_0,y_1,\cdots,y_9)$，其中 $y_d=1$，其余全部为 $0$；将灰度值除以 $255$，映射到范围 $[0,1]$ 中；用 $10$ 个独立的线性模型来拟合 $y_0,y_1,\cdots,y_9$，在预测时，取
$$
\hat{d}=\arg \max_i \hat{y}_i
$$
作为预测结果；在训练时，每次随机取一个数据，用梯度下降法进行迭代。

首先测试多元线性回归，$w$ 与 $b$ 在范围 $[-0.5,0.5]$ 内以均匀分布进行随机初始化，学习率 $\eta=0.001$，训练足够多次后，准确率达到 $83\%$ 左右。

![](C:\git\bs\figure\多元线性回归准确率.jpg)

然后测试对数几率回归，$w$ 与 $b$ 在范围 $[-0.5,0.5]$ 内以均匀分布进行随机初始化，学习率 $\eta=0.1$，训练足够多次后，准确率达到 $90\%$ 左右。

![](C:\git\bs\figure\对数几率回归准确率.jpg)

为了了解模型的权重分布情况，我们将 $w$ 进行可视化，下图是多元线性回归的权重图，权重越大，对应位置的颜色越亮

![](C:\git\bs\figure\多元线性回归权值.BMP)

从图中可以看出，在图像边缘部分权重的分布看起来杂乱无章，是因为图像的分类由中心位置的灰度值决定。

下图是对数几率回归的权重分布图，在这张图中可以隐约看到 $10$ 个数字的轮廓，不同数字的边缘部分明暗不一是由模型中常数项 $b$ 的不同导致的。

![](C:\git\bs\figure\对数几率回归权值.jpg)

## 神经网络模型

我们将线性模型抽象成神经网络模型，线性模型可以视为 $n+1$ 个神经元构成的神经网络，包括 $n$ 个输入神经元构成的输入层，与 $1$ 个输出神经元构成的输出层，每个神经元有输入值和输出值，这些值通过神经网络上的有向边来传递。两个线性模型的唯一不同之处是对数几率回归的输出神经元上添加了 sigmoid 函数。

![](C:\git\bs\figure\simple_net.jpg)

### 感知机

将线性模型变得稍加复杂一点，在输入层与输出层之间添加隐含层，包含若干个神经元，将输入层、隐含层、输出层的输出值分别记为 $A_0$、$A_1$、$A_2$，隐含层的每个神经元是输入层神经元输出值的线性组合+该神经元的阈值，即 $A_1=W_1A_0+b_1$，输出层神经元是隐含层神经元输出值的线性组合+该神经元的阈值，即 $A_2=W_2A_1+b_2$。当然，输出层可以包含多个神经元，也就可以输出多个值。这样的神经网络模型称为**感知机**。

![](C:\git\bs\figure\感知机.jpg)

然而，这样的模型仅仅是在结构上变得复杂了一点，输出的结果仍然是输入值的线性组合+阈值，效果与线性模型无异，为此，需要在隐含层与输出层神经元上添加激活函数，让模型的表达式变成非线性表达式。

#### 激活函数

在对数几率回归中的 sigmoid 函数就是一个激活函数，添加了激活函数的神经元，其输出值为输入值作用激活函数之后的值，常用的激活函数有

| 激活函数     | 函数表达式                               |
| ------------ | ---------------------------------------- |
| sigmoid 函数 | $f(x)=\frac{1}{1+e^{-x}}$                |
| relu 函数    | $f(x)=\max(x,0)$                         |
| softmax 函数 | $f(x_i)=\frac{e^{x_i}}{\sum_{j}e^{x_j}}$ |
| tanh 函数    | $f(x)=\frac{e^x-e^{-x}}{x^x+e^{-x}}$     |

#### 损失函数

在线性模型中，我们使用均方误差作为损失函数，来衡量模型参数的优劣，常用的损失函数有

| 损失函数 | 函数表达式                            |
| -------- | ------------------------------------- |
| 均方误差 | $E(y,\hat y)=\frac{1}{2}|y-\hat y|^2$ |
| 绝对误差 | $E(y,\hat y)=|y-\hat y|$              |
| 交叉熵   | $E(y,\hat y)=-y\ln \hat y$            |

其中的交叉熵一般和 softmax 函数用在单分类问题（例如手写数字识别）的输出层上，损失函数的不同会影响模型的收敛速度。

#### 误差逆传播算法

对于神经网络模型，在训练过程中，继续使用梯度下降法，考虑到模型结构的复杂性，我们把神经网络抽象成一个有向无环图 $G=(V,E)$，其中 $V$ 是所有神经元的集合，$E$ 是所有有向边的集合，我们可以先计算损失函数对于每个神经元输出值的偏导数，进而求出损失函数对每个权重和阈值的偏导数。

**定义 拓扑排序** 在有向无环图 $G=(V,E)$ 中，设 $P=(u_1,u_2,\cdots,u_n)$ 是顶点集 $V$ 的一个排列，若任意 $<u,v>\in E$，$u$ 在 $P$ 中排在 $v$ 前面，则称 $P$ 是 $G$ 的一个拓扑排序。

模型训练过程是将值沿图的拓扑排序传播，训练过程则需要反向将梯度（偏导数）沿反向图的拓扑排序传播，模型训练过程的伪代码如下：

```python
# 正向传播
对 V 进行拓扑排序
for u in V:
    for e = <u, v> in E:
        传递 u 的输出值到 v 的输入值
# 将所有边反向
for e = <u, v> in E:
    e = <v, u> 
# 反向传播
对 V 进行拓扑排序
计算损失函数对输出层神经元输出值的导数值
for u in V:
    for e = <u, v> in E:
        传递 u 的导数值到 v 的导数值
        更新 e 的权重
    更新 u 的阈值（如果有的话）
```

#### 使用单隐层感知机进行手写数字识别

以手写数字识别为例，在隐含层构造 $30$ 个神经元，输出层构造 $10$ 个神经元，在隐含层与输出层上添加 sigmoid 函数作为激活函数，所有权重与阈值在范围 $[-0.5,0.5]$ 中以均匀分布随机初始化，学习率 $\eta=0.1$，损失函数取均方误差，经过足够多次迭代后，准确率达到 $94\%$ 左右。

![](C:\git\bs\figure\感知机准确率.jpg)

单隐层感知机相比于线性模型，在准确率上有了提升，但是模型的可解释性降低，我们无法通过模型中的权重和阈值来解释模型的训练结果。

### 卷积神经网络（CNN）

#### 卷积层与池化层

在手写数字识别问题中，我们将数据抽象成了 $784$ 维的向量，而忽略了图像的二维结构，为了解决二维结构问题，我们引入两个特殊的神经网络层——**卷积层**和**池化层**

设图像的宽度为 $w_1$，高度为 $h_1$，那么图像的灰度值可以用矩阵表示
$$
P=\begin{pmatrix}
p_{11}&p_{12}&\cdots&p_{1w_1}\\
p_{21}&p_{22}&\cdots&p_{2w_1}\\
\vdots&\vdots&\ddots&\vdots\\
p_{h_11}&p_{h_12}&\cdots&p_{h_1w_1}
\end{pmatrix}
$$
卷积层包含 $h_2\times w_2$ 的权重矩阵 $C$ 与阈值 $b$
$$
C=\begin{pmatrix}
c_{11}&c_{12}&\cdots&c_{1w_2}\\
c_{21}&c_{22}&\cdots&c_{2w_2}\\
\vdots&\vdots&\ddots&\vdots\\
c_{h_21}&c_{h_22}&\cdots&c_{h_2w_2}
\end{pmatrix}
$$
卷积结果为 $h_3\times w_3(h_3=h_1-h_2+1,w_3=w_1-w_2+1)$ 的矩阵 $A=\{a_{ij}\}_{h_3\times w_3}$，其中
$$
a_{d_1d_2}=\sum_{i=1}^{h_2}\sum_{j=1}^{w_2}p_{d_1+i-1,d_2+j-1}c_{ij}+b
$$
$d_1=1,2,\cdots,h_3;d_2=1,2,\cdots,w_3$

![](C:\git\bs\figure\卷积层.jpg)

卷积层可以将图像某个区域内的信息保留，生成新的二维数据 $A$。

池化层通常连接在卷积层之后，用来解决图像过大的问题，适当缩小图像的尺寸，减少算力的消耗。常见的有最大值池化层，最大值池化层不含权值与阈值，它在二维数据的大小为 $h_4\times w_4$ 的区域内取最大值，这些最大值构成大小为 $h_5\times w_5(h_5=\frac{h_3}{h_4},w_5=\frac{w_3}{w_4})$ 的二维数据 $M=\{m_{ij}\}_{h_5\times w_5}$，其中
$$
m_{d_1d_2}=\max_{i=1,2,\cdots,h_4;j=1,2,\cdots,w_4}a_{(i-1)h_4+1,(j-1)w_4+1}
$$
（注：这里默认 $h_5=\frac{h_3}{h_4},w_5=\frac{w_3}{w_4}$ 是整数，如果不是，那就向下取整，但因此会损失一部分数据信息）

![](C:\git\bs\figure\池化层.jpg)

#### 使用卷积神经网络进行手写数字识别

我们构造一个简单的卷积神经网络来测试其效果，网络结构如下图所示：

![](C:\git\bs\figure\CNN.jpg)

原始图像分别经过 $10$ 个 $5\times 5$ 的卷积层，变成 $10$ 个 $24\times 24$ 的矩阵，每个矩阵经过对应的 $2\times 2$ 最大值池化层，变成 $10$ 个 $12\times 12$ 的矩阵即 $1440$ 个值，池化层作用 sigmoid 函数后与输出层的 $10$ 个神经元全连接，输出层作用 softmax 函数后输出结果。

在本神经网络中，卷积层包含 $(5\times 5+1)\times10=260$ 个参数，池化层与输出层之间包含 $1440\times 10=14400$ 个参数，输出层包含 $10$ 个阈值参数，共 $14670$ 个参数，每个参数在范围 $[-0.5,0.5]$ 内以均匀分布进行随机初始化，损失函数取交叉熵，学习率 $\eta=0.05$，经过足够多次训练后，准确率达到 $97\%$。

![](C:\git\bs\figure\CNN准确率.jpg)

### 循环神经网络（RNN）

#### 沿时间轴传播的循环神经网络

在现实生活中，我们有时需要对序列化的数据进行建模预测，例如某地点的气温变化，对于这类数据，可以构造结构特殊的神经网络来进行建模预测，**卷积神经网络**就是一个典型的用于序列化数据预测的神经网络。

![](C:\git\bs\figure\RNN.jpg)

在感知机中，将输入层、隐含层、输出层的输出值分别记为 $A$、$B$、$C$，设隐含层与输出层的激活函数分别为 $f,g$，输入层与隐含层之间的权重和阈值为 $(W_1,b_1)$，隐含层与输出层之间的权重和阈值为 $(W_2,b_2)$，那么有 $B=f(W_1A+b_1)$，$C=g(W_2B+b_2)$。现在考虑将上述模型应用到时间序列上，将 $t$ 时刻各层的输出值记为 $A_t$、$B_t$、$C_t$，如上图所示，添加新的连接关系，连接 $t-1$ 时刻的隐含层与 $t$ 时刻的隐含层，权重为 $W_3$，即 $B_t=f(W_1A_t+W_3B_{t-1}+b_1)$，$C_t=g(W_2B_t+b_2)$，这样就构成了一个简单的循环神经网络，该神经网络可以展开成下图的形式，需要注意的是，图中每个时刻的参数都是共享的。

![](C:\git\bs\figure\RNN_expand.jpg)

在使用 RNN 模型时，一般选取固定长度为 $T$ 的一段序列进行训练和预测。在计算损失函数值时，通常将这段序列每一个时间点的损失函数值相加，作为一次训练的损失函数值。在训练过程中，误差沿时间轴逆向传播，一种简单的实现方法是：先将不同时间点的神经元视为参数不共享的神经元，以学习率 $\eta$ 训练后，每个参数取该参数在所有时间点的平均值，即相当于以学习率 $\frac{\eta}{T}$ 进行了一次训练。

#### 长短期记忆网络（LSTM）

下面我们介绍一种结构较为复杂的神经网络——**长短期记忆网络（LSTM）**，它的结构如下所示：

![LSTM](C:\git\bs\figure\LSTM.jpg)

LSTM 的网络结构中含有两个沿时间轴传播的路径，即图中的 $C_2\to C_0$ 与 $H_1\to H_0$，模型按照下面的步骤进行正向传播：

**step 1**：计算 $C_1$

$X$ 是当前时间点的输入向量，$X$ 与 $D_0,D_1,D_2,D_3$ 之间有权重矩阵 $W_{XD_0},W_{XD_1},W_{XD_2},W_{XD_3}$；$H_0$ 来自上一时间点的 $H_1$，$H_1$ 与 $D_0,D_1,D_2,D_3$ 之间有权重矩阵 $W_{H_1D_0},W_{H_1D_1},W_{H_1D_2},W_{H_1D_3}$；$D_0,D_1,D_2,D_3$ 上有阈值 $b_{D_0},b_{D_1},b_{D_2},b_{D_3}$，其激活函数分别为 $\text{sigmoid},\text{sigmoid},\text{tanh},\text{sigmoid}$
$$
D_0=\text{sigmoid}(W_{XD_0}X+W_{H_0D_0}H_0+b_{D_0})
$$

$C_0$ 来自上一时间点的 $C_2$，$C_1$ 为 $C_0$ 与 $D_0$ 的乘积

$$
C_1=C_0*D_0
$$

这里的 $*$ 表示矩阵对应位置上的元素相乘

**step 2**：计算 $C_2$
$$
D_1=\text{sigmoid}(W_{XD_1}X+W_{H_0D_1}H_0+b_{D_1})
$$

$$
D_2=\tanh(W_{XD_2}X+W_{H_0D_2}H_0+b_{D_2})
$$

$$
M=D_1*D_2
$$

$$
C_2=C_1+M
$$

此外，还需要将此处 $C_2$ 的值传递到下一时间点的 $C_0$ 上

**step 3**：计算 $H_1$

$C_3$ 上有激活函数 $\text{sigmoid}$ 函数，有阈值 $b_{C_3}$
$$
C_3=\text{sigmoid}(C_2+b_{C_3})
$$

$$
D_3=\text{sigmoid}(W_{XD_3}X+W_{H_0D_3}H_0+b_{D_3})
$$

$$
H_1=C_3*D_3
$$

此外，还需要将此处 $H_1$ 的值传递到下一时间点的 $H_0$ 上

**step 4**：计算输出值 $Y$

当前时间点的输出值 $Y$ 由 $H_1$ 计算得出，即
$$
Y=f(H_1)
$$
这里的函数 $f$ 根据模型的具体用途来确定

#### 使用 LSTM 进行天气预测

我们通过网站 UCI Machine Learning Repository（https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data）获取到了北京市的天气数据集，该数据集包含了 2010 年到 2014 年北京市每小时的的 pm2.5、气温、气压等数据，我们只取其中每小时的气温，共有 $43824$ 条数据，取前 $34000$ 条数据作为训练数据，其余作为测试数据

在训练前，先对数据进行标准化处理，设时间点 $i$ 的气温为 $x_i$，令
$$
x_i\leftarrow \frac{x_i-\mu}{\sigma}
$$
其中 $\mu$ 为 $x$ 的平均值，$\sigma$ 为 $x$ 的标准差，这样可以使数据的平均值变为 $0$，方差变为 $1$

在构建模型时，每个时间点的输入向量由之前连续 $30$ 个时间点的气温构成，LSTM 隐含层的大小（$D_0,D_1,D_2,D_3,M,H_0,H_1,C_0,C_1,C_2$ 的神经元个数）为 $20$，$C_1$ 神经元与 $Y$ 全连接，损失函数取均方误差，学习率 $\eta=0.00125$，每次在训练数据集中随机取一段长度为 $T=40$ 的连续序列进行训练。

经过 $200000$ 次训练，得到训练结果，截取预测刚开始的一段，结果如下图所示：

![LSTM气温预测曲线](C:\git\bs\figure\LSTM气温预测曲线.jpg)

从图中可以看出，在刚开始预测的时候，预测气温曲线与真实气温曲线是非常接近的，随着时间的延后，误差会越来越大。预测气温曲线的振幅会逐渐变小，当经过足够长的时间后，预测气温曲线接近一条直线（篇幅原因，图中未显示）。

### 生成对抗网络（GAN）

## 参考文献

## 附录



