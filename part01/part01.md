## 线性模型

### 多元线性回归

#### 多元线性回归模型

现实生活中有很多随机变量之间是有内在联系的，比如人的身高和体重，一般来说，身高较高的人体重往往越重，但二者之间并没有确定的关系，回归即找到变量之间的潜在联系，得到其大致的变化趋势

假设有训练数据集 $\{x_i,y_i\}_{i=1}^m$ ，其中随机变量 $x_i\in \mathbb{R}^n,y_i\in \mathbb{R},i=1,2,\dots,m$ ，以及预测数据集 $\{x_i'\}_{i=1}^m$ ，我们需要得到预测数据集中每一个数据对应的 $y_i'$ 

在多元线性回归模型中， 假设 $x=(x_1,x_2,\dots,x_n)\in \mathbb{R}^n$ 与 $y\in \mathbb{R}$ 有关系
$$
y=\sum_{i=1}^nw_ix_i
$$
模型的训练过程就是根据 $x,y$ 求 $w$ ，预测过程就是根据 $w,x$ 求 $y$ 

#### 参数的确定

在训练过程中，我们要确定 $w$ ，使得误差的平方和（简称均方误差）最小，即
$$
\min\sum_{i=1}^m\left(\sum_{j=1}^nw_jx_{ij}-y_i\right)^2
$$
使用数学方法可以求出最优的 $w$ ，但是在实际应用中，由于数据量较大，很那用数学结论求得精确地最优解，因此可以使用梯度下降法得到 $w$ 的近似最优解

令目标函数
$$
E=\left(\sum_{j=1}^nw_jx_{ij}-y_i\right)^2
$$
计算 $E$ 对 $w$ 的梯度
$$
\frac{\partial E}{\partial w}=\left(\frac{\partial E}{\partial w_1},\frac{\partial E}{\partial w_2},\dots,\frac{\partial E}{\partial w_n}\right)=(2Ex_{i1},2Ex_{i2},\dots,2Ex_{in})
$$
梯度是函数变化最快的方向，向梯度的反方向移动 $w$ ，将会逐渐逼近最优解，即先随机确定 $w^{[0]}$ ，再以下面的公式进行迭代
$$
w^{[n+1]}=w^{[n]}-\eta\frac{\partial E}{\partial w^{[n]}}
$$
其中 $\eta$ 是常参数，称为学习率，具体数值根据实际问题调整

### 对数几率回归

#### 对数几率回归模型

线性回归模型原理简单，但是不够灵活，难以解决较为复杂的问题，在此基础上，有对数几率回归（又称逻辑回归），来让线性模型能够更好地解决分类问题

假设有训练数据集 $\{x_i,y_i\}_{i=1}^m$ ，其中随机变量 $x_i\in \mathbb{R}^n,y_i\in \{1,0\},i=1,2,\dots,m$ （ $y_i$ 表示数据的类别标记），以及预测数据集 $\{x_i'\}_{i=1}^m$ ，我们需要得到预测数据集中每一个数据对应的 $y_i'$ 

我们可以对数据属于两类的条件概率的比值进行预测，假设
$$
\ln \frac{P(y=1|x)}{P(y=0|x)}=\sum_{i=1}^nw_ix_i
$$
同时显然有
$$
P(y=1|x)+P(y=0|x)=1
$$
解得
$$
P(y=1|x)=\frac{e^{wx^T}}{1+e^{wx^T}}
$$

$$
P(y=0|x)=\frac{1}{1+e^{wx^T}}
$$

这样就能得到数据属于不同类别的概率

#### 参数的确定

与前面类似，需要构造目标函数来作为衡量 $w$ 的标准，与前面不同，不用均方误差，而是用似然概率的对数（这种方法又叫极大似然法），即
$$
\min\sum_{i=1}^m\ln P(y_i|x_i,w)
$$
取对数而不是原数值是为了让概率的乘法变成对数的加法，让计算变得简单

令
$$
E=\ln P(y_i|x_i,w)
$$
以 $y_i=0$ 为例，有
$$
\frac{\partial E}{\partial w_j}=-\frac{e^{wx^T}x_j}{1+e^{wx^T}}
$$
用同样的方法迭代即可得到 $w$ 